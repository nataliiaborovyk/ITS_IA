{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4febba9f",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier ‚Äî Versione Didattica\n",
    "\n",
    "Questo notebook contiene una versione **semplice e didattica** del classificatore Naive Bayes,\n",
    "insieme a un piccolo **dataset di fiori** per testarlo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefda8a",
   "metadata": {},
   "source": [
    "## üìò 1. Definizione del modello Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39815bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNaiveBayes:\n",
    "    \"\"\"\n",
    "    CLASSIFICATORE NAIVE BAYES (versione didattica)\n",
    "\n",
    "    Questo modello impara a classificare esempi in base alle loro caratteristiche\n",
    "    usando le probabilit√†. Non fa supposizioni complicate: considera\n",
    "    \"naivamente\" che ogni caratteristica sia indipendente dalle altre.\n",
    "\n",
    "    L'idea √®:\n",
    "        P(Classe | Caratt_i)  ‚àù  P(Classe) √ó P(Caratteristica_1 | Classe) √ó ...\n",
    "\n",
    "    Questa classe implementa esattamente questo concetto con passaggi molto semplici.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Inizializzazione dell'oggetto.\n",
    "        \n",
    "        Creiamo due strutture dati fondamentali:\n",
    "        - self.prior ‚Üí contiene P(Classe)\n",
    "        - self.feature_probs ‚Üí contiene P(feature | classe)\n",
    "        \n",
    "        Esempio struttura finale:\n",
    "        prior = {\"rosa\": 0.33, \"girasole\": 0.33, \"margherita\": 0.33} quindi quanto spesso compare ogni classe, ad es per i fiori, quante rose?\n",
    "\n",
    "        feature_probs = {\n",
    "            \"rosa\": [\n",
    "                {\"rosso\": 1.0, \"giallo\": 0.0},           # Probabilit√† del colore dato \"rosa\"\n",
    "                {\"tonda\": 1.0, \"allungata\": 0.0}         # Probabilit√† della forma dato \"rosa\"\n",
    "            ],\n",
    "            \"margherita\": [...],\n",
    "            \"girasole\": [...]\n",
    "            contiene quanto spesso una certa caratteristica appare\n",
    "           dentro una classe.\n",
    "           Tipo: di tutte le rose, quante sono rosse?\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        self.prior = {}             # Probabilit√† delle classi (P(classe))\n",
    "        self.feature_probs = {}     # Probabilit√† delle feature (P(feature | classe))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#                               FIT (ADDESTRAMENTO)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Addestra il modello sui dati.\n",
    "\n",
    "        X: lista di tuple che rappresentano le caratteristiche\n",
    "           Esempio: [(\"rosso\", \"tonda\"), (\"giallo\", \"allungata\"), ...]\n",
    "\n",
    "        y: lista delle classi corrispondenti\n",
    "           Esempio: [\"rosa\", \"girasole\", \"margherita\", ...]\n",
    "\n",
    "        L'obiettivo dell'addestramento √®:\n",
    "            1) Contare quante volte compare ogni classe ‚Üí P(Classe)\n",
    "            2) Per ogni classe, contare come sono distribuiti i valori\n",
    "               delle feature ‚Üí P(Feature | Classe)\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(y)                  # Numero totale di esempi\n",
    "        classi = set(y)             # Le classi distinte (es: {\"rosa\", \"margherita\", \"girasole\"}) senza duplicati\n",
    "        n_features = len(X[0])      # Numero di caratteristiche (feature)\n",
    "\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 1. CALCOLO DELLE PROBABILIT√Ä A PRIORI: P(Classe)\n",
    "        # -------------------------------------------------------------------------\n",
    "        # P(classe)\n",
    "        for cls in classi:                      # Quanti esempi appartengono a questa classe? es quante volte compare \"rosa\" nella lista y?\n",
    "            self.prior[cls] = y.count(cls) / n  # P(classe) = frequenza relativa - es quante volte compare \"rosa\" nella lista y?\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 2. CALCOLO DELLE PROBABILIT√Ä CONDIZIONATE: P(feature | classe)\n",
    "        # -------------------------------------------------------------------------\n",
    "        # P(feature | classe)\n",
    "        for cls in classi:\n",
    "\n",
    "            # Prepara una lista vuota: un dizionario per ogni feature\n",
    "            # Esempio: per 2 feature ‚Üí [ {}, {} ]\n",
    "\n",
    "            self.feature_probs[cls] = [ {} for _ in range(n_features) ]\n",
    "\n",
    "            # Seleziona SOLO gli esempi della classe corrente\n",
    "            # Esempio: per \"rosa\" prendo solo gli X[i] dove y[i] == \"rosa\"\n",
    "\n",
    "            X_cls = [X[i] for i in range(n) if y[i] == cls]\n",
    "            n_cls = len(X_cls)\n",
    "\n",
    "            # Per ogni caratteristica\n",
    "\n",
    "            for f_idx in range(n_features):\n",
    "\n",
    "                # Prendo i valori della feature f_idx per questa classe\n",
    "                # Esempio: per feature \"colore\" ‚Üí [\"rosso\", \"rosso\"] se entrambe le rose sono rosse\n",
    "\n",
    "                valori = [sample[f_idx] for sample in X_cls]\n",
    "\n",
    "                # Considero ogni valore possibile\n",
    "                for v in set(valori):\n",
    "                    # P(v | cls) = numero di volte che v compare / numero esempi classe\n",
    "                    self.feature_probs[cls][f_idx][v] = valori.count(v) / n_cls\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #                               PREVISIONE\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Qui il modello cerca di capire A QUALE CLASSE APPARTIENE\n",
    "        un nuovo esempio x (es. un fiore).\n",
    "        Calcola P(Classe | x) per OGNI classe.\n",
    "\n",
    "        x: nuovo esempio da classificare (es: (\"giallo\", \"tonda\"))\n",
    "\n",
    "        Procedura:\n",
    "            Per ogni classe:\n",
    "                1. Parto da P(Classe)\n",
    "                2. Moltiplico per ogni P(feature | classe)\n",
    "                3. Se un valore non √® stato mai visto ‚Üí smoothing (0.001)\n",
    "            Alla fine normalizzo tutto per ottenere probabilit√† vere che sommano a 1.\n",
    "        \"\"\"\n",
    "        post = {}                   # Dizionario che conterr√† le probabilit√† non normalizzate\n",
    "        for cls in self.prior:\n",
    "            # 1) Partenza: P(Classe)\n",
    "            prob = self.prior[cls]\n",
    "\n",
    "            # 2) Moltiplicazione delle probabilit√† condizionate\n",
    "            for f_idx, valore in enumerate(x):\n",
    "                # Se abbiamo gi√† visto il valore durante l'addestramento:\n",
    "                if valore in self.feature_probs[cls][f_idx]:\n",
    "                    prob *= self.feature_probs[cls][f_idx][valore]\n",
    "                else:\n",
    "                    # Valore mai visto ‚Üí Laplace smoothing molto semplice\n",
    "                    # Serve a evitare che una sola probabilit√† zero annulli tutto.\n",
    "                    prob *= 0.001\n",
    "            post[cls] = prob\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        #  NORMALIZZAZIONE\n",
    "        #  Convertiamo le probabilit√† ‚Äúgrezze‚Äù in probabilit√† vere che sommano a 1.\n",
    "        # -----------------------------------------------------------------------\n",
    "        tot = sum(post.values())\n",
    "        return {cls: post[cls] / tot for cls in post}\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #                 RESTITUISCE SOLO LA CLASSE PI√ô PROBABILE\n",
    "    # -------------------------------------------------------------------------\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Restituisce la classe con probabilit√† pi√π alta.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(x)\n",
    "        return max(probs, key=probs.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c7343d",
   "metadata": {},
   "source": [
    "## 2. Dataset dei fiori\n",
    "Esempio semplice con 3 classi: rosa, margherita, girasole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10f2e901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('rosso', 'tonda'),\n",
       "  ('bianco', 'tonda'),\n",
       "  ('giallo', 'allungata'),\n",
       "  ('giallo', 'tonda'),\n",
       "  ('rosso', 'tonda'),\n",
       "  ('giallo', 'allungata')],\n",
       " ['rosa', 'margherita', 'girasole', 'margherita', 'rosa', 'girasole'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset didattico dei fiori\n",
    "X = [\n",
    "    (\"rosso\",  \"tonda\"),\n",
    "    (\"bianco\", \"tonda\"),\n",
    "    (\"giallo\", \"allungata\"),\n",
    "    (\"giallo\", \"tonda\"),\n",
    "    (\"rosso\",  \"tonda\"),\n",
    "    (\"giallo\", \"allungata\")\n",
    "]\n",
    "\n",
    "y = [\"rosa\", \"margherita\", \"girasole\", \"margherita\", \"rosa\", \"girasole\"]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67c239",
   "metadata": {},
   "source": [
    "##  3. Addestriamo il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40e3f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello addestrato!\n"
     ]
    }
   ],
   "source": [
    "clf = SimpleNaiveBayes()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Modello addestrato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883e182",
   "metadata": {},
   "source": [
    "##  4. Facciamo una previsione\n",
    "Fiore nuovo: **giallo, tonda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d6f6c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilit√†: {'rosa': 0.00199203187250996, 'margherita': 0.99601593625498, 'girasole': 0.00199203187250996}\n",
      "Classe predetta: margherita\n"
     ]
    }
   ],
   "source": [
    "nuovo = (\"giallo\", \"tonda\")\n",
    "\n",
    "print(\"Probabilit√†:\", clf.predict_proba(nuovo))\n",
    "print(\"Classe predetta:\", clf.predict(nuovo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f92c3-babf-4709-b776-03a2c095be73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
